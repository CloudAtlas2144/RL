{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "\n",
        "<p><img height=\"80px\" src=\"https://www.upm.es/sfs/Rectorado/Gabinete%20del%20Rector/Logos/UPM/Escudo/EscUpm.jpg\" align=\"left\" hspace=\"0px\" vspace=\"0px\"></p>\n",
        "\n",
        "**Course \"Artificial Neural Networks and Deep Learning\" - Universidad Politécnica de Madrid (UPM)**\n",
        "\n",
        "# **Deep Q-Learning for Cartpole**\n",
        "\n",
        "This notebook includes an implementation of the Deep Q-learning (DQN) algorithm for the taxi problem (see [Taxi documentation](https://gymnasium.farama.org/environments/toy_text/taxi/)).\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Autores:\n",
        "\n",
        "Sergio Arroni del Riego (s.arroni@alumnos.upm.es)\n",
        "\n",
        "David González Fernández (david.gonzalezf@alumnos.upm.es)\n",
        "\n",
        "José Manuel Lamas Pérez (jm.lperez@alumnos.upm.es)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JXBzOdaLAEUn"
      },
      "source": [
        "##Libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 134,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "from os import system\n",
        "import time\n",
        "from itertools import count\n",
        "import numpy as np\n",
        "from pathlib import Path\n",
        "import gymnasium as gym\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from tensorflow import keras\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.nn.functional import smooth_l1_loss as huber_loss\n",
        "import torch.optim as optim\n",
        "\n",
        "from collections import namedtuple\n",
        "from tqdm import trange"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nBrRuhN1AQ-s"
      },
      "source": [
        "## Hyperparameters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 135,
      "metadata": {
        "id": "4N2yVwtuFlBu"
      },
      "outputs": [],
      "source": [
        "GAMMA = 0.99\n",
        "MEMORY_SIZE = 50000\n",
        "LEARNING_RATE = 0.001\n",
        "BATCH_SIZE = 128\n",
        "EXPLORATION_MAX = 1\n",
        "EXPLORATION_MIN = 0.001\n",
        "EXPLORATION_DECAY = 400\n",
        "NUMBER_OF_EPISODES_FOR_TRAINING = 3000\n",
        "NUMBER_OF_EPISODES_FOR_TESTING = 30\n",
        "TARGET_MODEL_UPDATE_EPISODES = 20\n",
        "MAX_STEPS_PER_EPISODE = 100"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 136,
      "metadata": {},
      "outputs": [],
      "source": [
        "Transition = namedtuple('Transition',\n",
        "                        ('state', 'action', 'next_state', 'reward', 'done'))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OoGaas6TAd6p"
      },
      "source": [
        "## Class ReplayMemory\n",
        "\n",
        "Memory of transitions for experience replay."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Cambios en la Implementación de la Memoria de Reproducción (Replay Memory)\n",
        "\n",
        "En la implementación original, la memoria de reproducción (`ReplayMemory`) estaba basada en matrices NumPy para almacenar las transiciones del agente. Se utilizaban múltiples matrices independientes para almacenar estados, acciones, recompensas, estados siguientes y estados terminales. A continuación, se describen los cambios realizados y sus justificaciones:\n",
        "\n",
        "## Uso de la Tupla con nombre `Transition`\n",
        "Se introdujo una tupla con nombre llamada `Transition` para representar una transición del entorno. Esta tupla incluye campos para el estado actual, la acción tomada, el siguiente estado, la recompensa obtenida y un indicador booleano para indicar si el estado siguiente es un estado terminal. El uso de la tupla con nombre hace que el código sea más legible y reduce la posibilidad de errores al acceder a los elementos de la transición.\n",
        "\n",
        "```python\n",
        "Transition = namedtuple('Transition', ('state', 'action', 'next_state', 'reward', 'done'))\n",
        "```\n",
        "\n",
        "## Cambio en el Almacenamiento de Transiciones\n",
        "En lugar de mantener varias matrices independientes, se utiliza una lista única llamada `memory` para almacenar las transiciones como instancias de la tupla `Transition`. Esta lista se inicializa vacía y se va llenando a medida que se almacenan nuevas transiciones. Cuando la capacidad máxima se alcanza, las nuevas transiciones reemplazan a las más antiguas en un ciclo circular, lo que mantiene la memoria limitada al tamaño especificado.\n",
        "\n",
        "```python\n",
        "def push(self, *args):\n",
        "    \"\"\"Saves a transition.\"\"\"\n",
        "    if len(self.memory) < self.capacity:\n",
        "        self.memory.append(None)\n",
        "    self.memory[self.position] = Transition(*args)\n",
        "    self.position = (self.position + 1) % self.capacity\n",
        "```\n",
        "\n",
        "## Muestreo Eficiente\n",
        "La función `sample_memory` fue reemplazada por `sample`, que utiliza una generación eficiente de índices aleatorios mediante `numpy.random.default_rng()`. Esto mejora la eficiencia y la velocidad del muestreo de la memoria de reproducción.\n",
        "\n",
        "```python\n",
        "def sample(self, batch_size):\n",
        "    idx = self.rng.choice(np.arange(len(self.memory)), batch_size, replace=False)\n",
        "    res = [self.memory[i] for i in idx]\n",
        "    return res\n",
        "```\n",
        "\n",
        "## Métodos Adicionales\n",
        "Se añadió el método `__len__` para obtener la longitud actual de la memoria de reproducción, lo que facilita realizar un seguimiento del tamaño actual.\n",
        "\n",
        "```python\n",
        "def __len__(self):\n",
        "    return len(self.memory)\n",
        "```\n",
        "\n",
        "Estos cambios buscan mejorar la claridad del código, la eficiencia y la facilidad de mantenimiento al adoptar mejores prácticas de Python y estructuras de datos más adecuadas. La implementación resultante es más legible y fácil de entender."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 137,
      "metadata": {
        "id": "cQV7IfhFOoSh"
      },
      "outputs": [],
      "source": [
        "class ReplayMemory(object):\n",
        "    def __init__(self, capacity):\n",
        "        self.capacity = capacity\n",
        "        self.memory = []\n",
        "        self.position = 0\n",
        "        self.rng = np.random.default_rng()\n",
        "\n",
        "    def push(self, *args):\n",
        "        \"\"\"Saves a transition.\"\"\"\n",
        "        if len(self.memory) < self.capacity:\n",
        "            self.memory.append(None)\n",
        "        self.memory[self.position] = Transition(*args)\n",
        "        self.position = (self.position + 1) % self.capacity\n",
        "\n",
        "    def sample(self, batch_size):\n",
        "        idx = self.rng.choice(np.arange(len(self.memory)), batch_size, replace=False)\n",
        "        res = []\n",
        "        for i in idx:\n",
        "            res.append(self.memory[i])\n",
        "        return res\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.memory)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gejKO0OYAsS4"
      },
      "source": [
        "## Class DQN \n",
        "\n",
        "La clase DQN se ha separado en dos: DQN y QAgent para que el código esté más organizado y se entienda mejor.\n",
        "\n",
        "La clase DQN contiene la implementación de la arquitectura utilizada. En `__init__` se definen las capas mientras que en la función `forward` se introducen las funciones de activación.\n",
        "\n",
        "## Class QAgent\n",
        "\n",
        "El constructor `__init__` del agente Q realiza la inicialización del objeto, configurando parámetros clave. Algunas de las configuraciones notables incluyen:\n",
        "\n",
        "Entorno (env): Se crea un entorno de Taxi con la biblioteca Gym.\n",
        "Modelo DQN (model_class): Se define la clase del modelo como DQN.\n",
        "Dispositivo de Ejecución (device): Se selecciona automáticamente el dispositivo GPU si está disponible; de lo contrario, se utiliza la CPU.\n",
        "Memoria de Reproducción (memory): Se inicializa como None y se asignará más adelante durante el entrenamiento.\n",
        "Directorio de Respaldo del Modelo (model_dir): Se establece un directorio para almacenar respaldos del modelo.\n",
        "Comprobación de Modelo Preentrenado (pt_path): Determina si se reanuda desde un modelo preentrenado o se inicia uno nuevo.\n",
        "\n",
        "## Compile\n",
        "\n",
        "El método `compile` compila el modelo DQN, inicializando el modelo y el optimizador. Algunas características destacadas son:\n",
        "\n",
        "Inicialización del Modelo (self.model): Se inicializa con la clase DQN y se carga un modelo preentrenado si está disponible.\n",
        "Inicialización del Modelo Objetivo (self.target_model): Se inicializa como una copia del modelo actual y se configura en modo de evaluación.\n",
        "Optimizador Adam (self.optimizer): Se inicializa con una tasa de aprendizaje específica.\n",
        "\n",
        "## Funciones Epsilon-Greedy\n",
        "\n",
        "Estas funciones implementan la política epsilon-greedy. `_get_epsilon` calcula el valor de epsilon basado en el número de episodios, mientras que `_choose_action` selecciona una acción según la política epsilon-greedy.\n",
        "\n",
        "## Entrenamiento del modelo\n",
        "\n",
        "La función `train_model` entrena el modelo DQN utilizando una muestra de la memoria de reproducción. Incluye pasos para calcular los valores Q predichos y esperados, calcular la pérdida y optimizar el modelo.\n",
        "\n",
        "## Update target\n",
        "\n",
        "Esta función actualiza la red objetivo copiando los pesos del modelo actual. Se realiza periódicamente para estabilizar el aprendizaje.\n",
        "\n",
        "## Remember\n",
        "\n",
        "Este método almacena una transición en la memoria de reproducción, encapsulando el estado, la acción, el próximo estado, la recompensa y una bandera de finalización.\n",
        "\n",
        "## Fit\n",
        "\n",
        "Este método implementa el ciclo de entrenamiento del agente Q, manejando interrupciones y mostrando el progreso del entrenamiento. Incluye pasos para la inicialización del entorno, selección de acciones, entrenamiento del modelo, actualización de la red objetivo y visualización del progreso.\n",
        "\n",
        "## Plot durations\n",
        "\n",
        "Este método genera gráficos para visualizar la duración de los episodios, las recompensas y los valores de epsilon a lo largo del entrenamiento.\n",
        "\n",
        "## Save\n",
        "\n",
        "Este método guarda el estado actual del modelo y del optimizador en un archivo de punto de control.\n",
        "\n",
        "## Moving Average\n",
        "\n",
        "Esta función calcula el promedio móvil de una secuencia para suavizar las curvas de los gráficos.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 138,
      "metadata": {},
      "outputs": [],
      "source": [
        "class DQN(nn.Module):\n",
        "    def __init__(self, number_of_observations, number_of_actions):\n",
        "        super(DQN, self).__init__()\n",
        "        self.emb = nn.Embedding(number_of_observations, 6)\n",
        "        self.l1 = nn.Linear(6, 50)\n",
        "        self.l2 = nn.Linear(50, 50)\n",
        "        self.l3 = nn.Linear(50, 50)\n",
        "        self.l4 = nn.Linear(50, 100)  \n",
        "        self.l5 = nn.Linear(100, number_of_actions)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = F.relu(self.l1(self.emb(x)))\n",
        "        x = F.relu(self.l2(x))\n",
        "        x = F.relu(self.l3(x))\n",
        "        x = F.relu(self.l4(x))  \n",
        "        x = self.l5(x)\n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 139,
      "metadata": {},
      "outputs": [],
      "source": [
        "from PIL import Image\n",
        "\n",
        "\n",
        "class QAgent:\n",
        "    def __init__(self, pt_path: str = None) -> None:\n",
        "        \"\"\"\n",
        "        Initializes a QAgent object.\n",
        "\n",
        "        Args:\n",
        "            pt_path (str, optional): Path to a pre-trained model. Defaults to None.\n",
        "        \"\"\"\n",
        "        # Initialize QAgent object with optional pre-trained model path\n",
        "        self.env = gym.make(\"Taxi-v3\").env\n",
        "        self.model_dir = Path(\"./model_backup\")\n",
        "        self.model_class = DQN\n",
        "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "        self.memory = None\n",
        "        self.rng = np.random.default_rng(42)\n",
        "        self.episode_durations = []\n",
        "        self.reward_in_episode = []\n",
        "        self.epsilon_vec = []\n",
        "        self.last_step = 0\n",
        "        self.last_episode = 0\n",
        "\n",
        "        # Create model backup directory if it doesn't exist\n",
        "        if not os.path.exists(self.model_dir):\n",
        "            os.makedirs(self.model_dir)\n",
        "\n",
        "        # Determine if resuming from a pre-trained model\n",
        "        self.is_resume = True if pt_path else False\n",
        "        self.pt_path = (\n",
        "            pt_path\n",
        "            if self.is_resume\n",
        "            else f\"{self.model_dir}/pytorch_{int(time.time())}.pt\"\n",
        "        )\n",
        "        self.checkpoint = torch.load(self.pt_path) if self.is_resume else None\n",
        "\n",
        "        # training\n",
        "        self.batch_size: int = BATCH_SIZE\n",
        "        self.learning_rate: float = LEARNING_RATE\n",
        "        self.loss = huber_loss\n",
        "        self.num_episodes: int = NUMBER_OF_EPISODES_FOR_TRAINING\n",
        "        self.train_steps: int = 1000000\n",
        "        self.warmup_episode: int = 0 if self.is_resume else 100\n",
        "        self.save_freq: int = 1000\n",
        "        # optimizer\n",
        "        self.lr_min: float = 0.0001\n",
        "        self.lr_decay: int = 5000\n",
        "        # rl\n",
        "        self.gamma: float = GAMMA\n",
        "        self.max_steps_per_episode: int = MAX_STEPS_PER_EPISODE\n",
        "        self.target_model_update_episodes: int = TARGET_MODEL_UPDATE_EPISODES\n",
        "        self.max_queue_length: int = MEMORY_SIZE\n",
        "        # epsilon\n",
        "        self.max_epsilon: float = EXPLORATION_MAX\n",
        "        self.min_epsilon: float = EXPLORATION_MIN\n",
        "        self.decay_epsilon: float = EXPLORATION_DECAY\n",
        "\n",
        "    def compile(self) -> None:\n",
        "        \"\"\"\n",
        "        Compiles the DQN model by initializing the model, loading the checkpoint if resume is enabled,\n",
        "        initializing the target model, setting it to evaluation mode, and initializing the optimizer.\n",
        "        \"\"\"\n",
        "        n_actions = self.env.action_space.n\n",
        "        number_of_observations = self.env.observation_space.n\n",
        "\n",
        "        self.model = self.model_class(number_of_observations, n_actions).to(self.device)\n",
        "        if self.is_resume:\n",
        "            self.model.load_state_dict(self.checkpoint[\"model_state_dict\"])\n",
        "        self.target_model = self.model_class(number_of_observations, n_actions).to(\n",
        "            self.device\n",
        "        )\n",
        "        self.target_model.load_state_dict(self.model.state_dict())\n",
        "        self.target_model.eval()\n",
        "        self.optimizer = optim.Adam(self.model.parameters(), lr=self.learning_rate)\n",
        "        if self.is_resume:\n",
        "            self.optimizer.load_state_dict(self.checkpoint[\"optimizer_state_dict\"])\n",
        "\n",
        "    def _get_epsilon(self, episode: int) -> float:\n",
        "        \"\"\"\n",
        "        Compute epsilon for epsilon-greedy policy.\n",
        "\n",
        "        Args:\n",
        "            episode (int): The current episode number.\n",
        "\n",
        "        Returns:\n",
        "            float: The computed epsilon value.\n",
        "        \"\"\"\n",
        "        return self.min_epsilon + (self.max_epsilon - self.min_epsilon) * np.exp(\n",
        "            -episode / self.decay_epsilon\n",
        "        )\n",
        "\n",
        "    def _get_action_for_state(self, state) -> int:\n",
        "        \"\"\"\n",
        "        Get action for a given state based on the current Q-network.\n",
        "\n",
        "        Args:\n",
        "            state: The current state.\n",
        "\n",
        "        Returns:\n",
        "            int: The selected action.\n",
        "        \"\"\"\n",
        "        with torch.no_grad():\n",
        "            f_state = state if type(state) == int else state[0]\n",
        "            predicted = self.model(torch.tensor([f_state], device=self.device))\n",
        "            action = predicted.max(1)[1]\n",
        "        return action.item()\n",
        "\n",
        "    def _choose_action(self, state, epsilon: float) -> int:\n",
        "        \"\"\"\n",
        "        Choose an action based on epsilon-greedy policy.\n",
        "\n",
        "        Args:\n",
        "            state: The current state.\n",
        "            epsilon (float): The epsilon value for exploration.\n",
        "\n",
        "        Returns:\n",
        "            int: The selected action.\n",
        "        \"\"\"\n",
        "        if self.rng.uniform() < epsilon:\n",
        "            # Explore\n",
        "            action = self.env.action_space.sample()\n",
        "        else:\n",
        "            # Exploit\n",
        "            action = self._get_action_for_state(state)\n",
        "        return action\n",
        "\n",
        "    def _adjust_learning_rate(self, episode: int) -> None:\n",
        "        \"\"\"\n",
        "        Adjust learning rate based on exponential decay.\n",
        "\n",
        "        Args:\n",
        "            episode (int): The current episode number.\n",
        "        \"\"\"\n",
        "        delta = self.learning_rate - self.lr_min\n",
        "        base = self.lr_min\n",
        "        rate = self.lr_decay\n",
        "        lr = base + delta * np.exp(-episode / rate)\n",
        "        for param_group in self.optimizer.param_groups:\n",
        "            param_group[\"lr\"] = lr\n",
        "\n",
        "    def _train_model(self) -> None:\n",
        "        \"\"\"\n",
        "        Train the Q-network using a batch of experiences from replay memory.\n",
        "        \"\"\"\n",
        "        if len(self.memory) < self.batch_size:\n",
        "            return\n",
        "        transitions = self.memory.sample(self.batch_size)\n",
        "        batch = Transition(*zip(*transitions))\n",
        "\n",
        "        state_batch = torch.cat(batch.state)\n",
        "        action_batch = torch.cat(batch.action)\n",
        "        reward_batch = torch.cat(batch.reward)\n",
        "        next_state_batch = torch.cat(batch.next_state)\n",
        "        done_batch = torch.cat(batch.done)\n",
        "\n",
        "        # Compute predicted Q values\n",
        "        predicted_q_value = self.model(state_batch).gather(1, action_batch.unsqueeze(1))\n",
        "\n",
        "        # Compute the expected Q values\n",
        "        next_state_values = self.target_model(next_state_batch).max(1)[0]\n",
        "        expected_q_values = (\n",
        "            ~done_batch * next_state_values * self.gamma\n",
        "        ) + reward_batch\n",
        "\n",
        "        # Compute loss\n",
        "        loss = self.loss(predicted_q_value, expected_q_values.unsqueeze(1))\n",
        "\n",
        "        # Optimize the model\n",
        "        self.optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        for param in self.model.parameters():\n",
        "            param.grad.data.clamp_(-1, 1)\n",
        "        self.optimizer.step()\n",
        "\n",
        "    def _update_target(self) -> None:\n",
        "        \"\"\"\n",
        "        Update the target Q-network by copying the weights from the current Q-network.\n",
        "        \"\"\"\n",
        "        self.target_model.load_state_dict(self.model.state_dict())\n",
        "\n",
        "    def _remember(self, state, action, next_state, reward, done) -> None:\n",
        "        \"\"\"\n",
        "        Store an experience tuple in replay memory.\n",
        "\n",
        "        Args:\n",
        "            state: The current state.\n",
        "            action: The selected action.\n",
        "            next_state: The next state.\n",
        "            reward: The reward received.\n",
        "            done: A flag indicating if the episode is done.\n",
        "        \"\"\"\n",
        "        f_state = state if type(state) == int else state[0]\n",
        "        self.memory.push(\n",
        "            torch.tensor([f_state], device=self.device),\n",
        "            torch.tensor([action], device=self.device, dtype=torch.long),\n",
        "            torch.tensor([next_state], device=self.device),\n",
        "            torch.tensor([reward], device=self.device),\n",
        "            torch.tensor([done], device=self.device, dtype=torch.bool),\n",
        "        )\n",
        "\n",
        "    def _get_loss(self) -> callable:\n",
        "        \"\"\"\n",
        "        Get the loss function for training the Q-network.\n",
        "\n",
        "        Returns:\n",
        "            callable: The loss function.\n",
        "        \"\"\"\n",
        "        return F.smooth_l1_loss\n",
        "\n",
        "    def fit(self) -> None:\n",
        "        \"\"\"\n",
        "        Fit the QAgent to the environment by training the Q-network.\n",
        "        \"\"\"\n",
        "        try:\n",
        "            self.loss = self._get_loss()\n",
        "            self.memory = ReplayMemory(50000)\n",
        "\n",
        "            self.episode_durations = []\n",
        "            self.reward_in_episode = []\n",
        "            self.epsilon_vec = []\n",
        "            reward_in_episode = 0\n",
        "            epsilon = 1\n",
        "\n",
        "            progress_bar = trange(\n",
        "                0, self.num_episodes, initial=self.last_episode, total=self.num_episodes\n",
        "            )\n",
        "\n",
        "            for i_episode in progress_bar:\n",
        "                # Initialize the environment and state\n",
        "                state = self.env.reset()\n",
        "                if i_episode >= self.warmup_episode:\n",
        "                    epsilon = self._get_epsilon(i_episode - self.warmup_episode)\n",
        "\n",
        "                for step in count():\n",
        "                    # Select and perform an action\n",
        "                    action = self._choose_action(state, epsilon)\n",
        "                    next_state, reward, done, truncated, _ = self.env.step(action)\n",
        "\n",
        "                    # Store the transition in memory\n",
        "                    self._remember(state, action, next_state, reward, done)\n",
        "\n",
        "                    # Perform one step of the optimization (on the target network)\n",
        "                    if i_episode >= self.warmup_episode:\n",
        "                        self._train_model()\n",
        "                        self._adjust_learning_rate(i_episode - self.warmup_episode + 1)\n",
        "                        done = (step == self.max_steps_per_episode - 1) or done\n",
        "                    else:\n",
        "                        done = (step == 5 * self.max_steps_per_episode - 1) or done\n",
        "\n",
        "                    # Move to the next state\n",
        "                    state = next_state\n",
        "                    reward_in_episode += reward\n",
        "\n",
        "                    if done:\n",
        "                        # Update episode statistics\n",
        "                        self.episode_durations.append(step + 1)\n",
        "                        self.reward_in_episode.append(reward_in_episode)\n",
        "                        self.epsilon_vec.append(epsilon)\n",
        "                        reward_in_episode = 0\n",
        "\n",
        "                        # Calculate and display progress\n",
        "                        N = min(10, len(self.episode_durations))\n",
        "                        progress_bar.set_postfix(\n",
        "                            {\n",
        "                                \"reward\": np.mean(self.reward_in_episode[-N:]),\n",
        "                                \"steps\": np.mean(self.episode_durations[-N:]),\n",
        "                                \"epsilon\": epsilon,\n",
        "                            }\n",
        "                        )\n",
        "\n",
        "                        break\n",
        "\n",
        "                # Update the target network, copying all weights and biases in DQN\n",
        "                if i_episode % self.target_model_update_episodes == 0:\n",
        "                    self._update_target()\n",
        "\n",
        "                if i_episode % self.save_freq == 0:\n",
        "                    self.save()\n",
        "\n",
        "                self.last_episode = i_episode\n",
        "\n",
        "            self.plot_durations()\n",
        "\n",
        "        except KeyboardInterrupt:\n",
        "            # Handle interruption and display training progress\n",
        "            self.plot_durations()\n",
        "            print(\"Training has been interrupted\")\n",
        "\n",
        "    def plot_durations(self) -> None:\n",
        "        \"\"\"\n",
        "        Plot training durations, rewards, and epsilon values.\n",
        "        \"\"\"\n",
        "        lines = []\n",
        "        plt.ion()\n",
        "        fig = plt.figure(1, figsize=(15, 7))\n",
        "        plt.clf()\n",
        "        ax1 = fig.add_subplot(111)\n",
        "\n",
        "        plt.title(\"Training...\")\n",
        "        ax1.set_xlabel(\"Episode\")\n",
        "        ax1.set_ylabel(\"Duration & Rewards\")\n",
        "        ax1.set_ylim(-2 * self.max_steps_per_episode, self.max_steps_per_episode + 10)\n",
        "        ax1.plot(self.episode_durations, color=\"C1\", alpha=0.2)\n",
        "        ax1.plot(self.reward_in_episode, color=\"C2\", alpha=0.2)\n",
        "        mean_steps = self._moving_average(self.episode_durations, periods=5)\n",
        "        mean_reward = self._moving_average(self.reward_in_episode, periods=5)\n",
        "        lines.append(ax1.plot(mean_steps, label=\"steps\", color=\"C1\")[0])\n",
        "        lines.append(ax1.plot(mean_reward, label=\"rewards\", color=\"C2\")[0])\n",
        "\n",
        "        ax2 = ax1.twinx()\n",
        "        ax2.set_ylabel(\"Epsilon\")\n",
        "        lines.append(ax2.plot(self.epsilon_vec, label=\"epsilon\", color=\"C3\")[0])\n",
        "        labs = [l.get_label() for l in lines]\n",
        "        ax1.legend(lines, labs, loc=3)\n",
        "\n",
        "        plt.draw()\n",
        "        plt.savefig(f\"./img/{int(time.time())}.png\")\n",
        "\n",
        "    def save(self) -> None:\n",
        "        \"\"\"\n",
        "        Save the current model state and optimizer state.\n",
        "        \"\"\"\n",
        "        torch.save(\n",
        "            {\n",
        "                # testing ready model save format\n",
        "                \"emb.weight\": self.model.emb.weight,\n",
        "                \"l1.weight\": self.model.l1.weight,\n",
        "                \"l1.bias\": self.model.l1.bias,\n",
        "                \"l2.weight\": self.model.l2.weight,\n",
        "                \"l2.bias\": self.model.l2.bias,\n",
        "                \"l3.weight\": self.model.l3.weight,\n",
        "                \"l3.bias\": self.model.l3.bias,\n",
        "                \"l4.weight\": self.model.l4.weight,\n",
        "                \"l4.bias\": self.model.l4.bias,\n",
        "                # resume training model save format\n",
        "                \"model_state_dict\": self.model.state_dict(),\n",
        "                \"optimizer_state_dict\": self.optimizer.state_dict(),\n",
        "                \"loss\": self.loss,\n",
        "            },\n",
        "            self.pt_path,\n",
        "        )\n",
        "\n",
        "    @staticmethod\n",
        "    def _moving_average(x, periods: int = 5) -> np.ndarray:\n",
        "        # Calculate the moving average of a sequence\n",
        "        if len(x) < periods:\n",
        "            return x\n",
        "        cumsum = np.cumsum(np.insert(x, 0, 0))\n",
        "        res = (cumsum[periods:] - cumsum[:-periods]) / periods\n",
        "        return np.hstack([x[: periods - 1], res])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 140,
      "metadata": {},
      "outputs": [],
      "source": [
        "def train(pt_path=None):\n",
        "    taxi = QAgent(pt_path)\n",
        "    taxi.compile()\n",
        "    taxi.fit()\n",
        "    return taxi.pt_path"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 141,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  0%|          | 6/3000 [00:00<03:02, 16.44it/s, reward=-2e+3, steps=500, epsilon=1]  "
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 29%|██▊       | 860/3000 [09:44<18:43,  1.90it/s, reward=-42.8, steps=48.2, epsilon=0.151] "
          ]
        }
      ],
      "source": [
        "pt_path = train()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'model_backup/pytorch_1704575404.pt'"
            ]
          },
          "execution_count": 129,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "pt_path"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "from IPython.display import display, clear_output\n",
        "from matplotlib.animation import FuncAnimation\n",
        "from time import sleep\n",
        "import imageio\n",
        "from gymnasium.utils.save_video import save_video"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class TestModel:\n",
        "    def __init__(self, model_class, model_file):\n",
        "        # Generate gymnasium environment variables\n",
        "        self.env = gym.make(\"Taxi-v3\", render_mode=\"ansi\").env\n",
        "        self.state, self.info = self.env.reset()\n",
        "        self.stop_anim = False\n",
        "\n",
        "        # define model related variables\n",
        "        self.model_class = model_class\n",
        "        self.path = model_file\n",
        "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "        self.n_actions = self.env.action_space.n\n",
        "        self.number_of_observations = self.env.observation_space.n\n",
        "\n",
        "        self.model = model_class(self.number_of_observations, self.n_actions).to(\n",
        "            self.device\n",
        "        )\n",
        "        checkpoint = torch.load(self.path)\n",
        "        self.model.load_state_dict(checkpoint[\"model_state_dict\"])\n",
        "        self.model.eval()\n",
        "\n",
        "    def _init_test_variables(\n",
        "        self, test_episodes, timestamp, fast_testing, final_frame_pause\n",
        "    ):\n",
        "        # test environment specific variables\n",
        "        self.penalties = 0\n",
        "        self.reward = 0\n",
        "        self.rewards = []\n",
        "        self.test_episodes = (\n",
        "            test_episodes  # Number of episode to run in test environment\n",
        "        )\n",
        "        self.window = None\n",
        "        self.done = False\n",
        "        # total results variables\n",
        "        self.total_epochs = 0\n",
        "        self.total_penalties = 0\n",
        "        # arguments related variables\n",
        "        self.timestamp = timestamp\n",
        "        self.fast_testing = fast_testing\n",
        "        self.final_frame_pause = final_frame_pause\n",
        "\n",
        "    def _get_action_for_state(self, state):\n",
        "        with torch.no_grad():\n",
        "            # t.max(1) will return largest column value of each row.\n",
        "            # second column on max result is index of where max element was\n",
        "            # found, so we pick action with the larger expected reward.\n",
        "            f_state = state if type(state) == int else state[0]\n",
        "            predicted = self.model(torch.tensor([f_state], device=self.device))\n",
        "            action = predicted.max(1)[1]\n",
        "        return action.item()\n",
        "\n",
        "    def test(\n",
        "        self, test_episodes=1, timestamp=0.2, fast_testing=False, final_frame_pause=0\n",
        "    ):\n",
        "        self._init_test_variables(\n",
        "            test_episodes, timestamp, fast_testing, final_frame_pause\n",
        "        )\n",
        "\n",
        "        for i in range(self.test_episodes):\n",
        "            state = self.env.reset()[0]\n",
        "            episode_reward, epochs, self.penalties, self.reward = 0, 0, 0, 0\n",
        "            self.done = False\n",
        "\n",
        "            while not self.done:\n",
        "                action = self._get_action_for_state(state)\n",
        "                state, self.reward, self.done, truncated, info = self.env.step(action)\n",
        "                episode_reward += self.reward\n",
        "                if self.reward == -10:\n",
        "                    self.penalties += 1\n",
        "                epochs += 1\n",
        "\n",
        "                self.display_test(state, action, episode_reward, i)\n",
        "                if epochs > 25:\n",
        "                    break\n",
        "\n",
        "            self.total_penalties += self.penalties\n",
        "            self.rewards.append(episode_reward)\n",
        "            self.total_epochs += epochs\n",
        "\n",
        "        results = open(\"results/test_results.txt\", \"a\")\n",
        "        results.write(\n",
        "            \"---------------------------------------------------------------------------------------------\\n\"\n",
        "        )\n",
        "        results.write(f\"Results after {self.test_episodes} episodes:\\n\")\n",
        "        results.write(\n",
        "            f\"Average timesteps per episode: {self.total_epochs / self.test_episodes}\\n\"\n",
        "        )\n",
        "        results.write(\n",
        "            f\"Average penalties per episode: {self.total_penalties / self.test_episodes}\\n\"\n",
        "        )\n",
        "        results.write(f\"Max score: {max(self.rewards)}\\n\")\n",
        "        results.write(f\"Min score: {min(self.rewards)}\\n\")\n",
        "        results.write(f\"Average score: {np.mean(self.rewards)}\\n\")\n",
        "        results.write(f\"Standard deviation: {np.std(self.rewards)}\\n\")\n",
        "        results.write(f\"Total timesteps: {self.total_epochs}\\n\")\n",
        "        results.write(f\"Total penalties: {self.total_penalties}\\n\")\n",
        "        results.write(f\"Total time: {self.timestamp * self.total_epochs}\\n\")\n",
        "        results.write(\n",
        "            \"---------------------------------------------------------------------------------------------\\n\"\n",
        "        )\n",
        "        results.close()\n",
        "\n",
        "    def display_test(self, state, action, episode_reward, i):\n",
        "        system(\"clear\")\n",
        "        if not self.fast_testing:\n",
        "            frame = {\n",
        "                \"frame\": self.env.render(),\n",
        "                \"state\": state,\n",
        "                \"action\": action,\n",
        "                \"reward\": self.reward,\n",
        "                \"episode_reward\": episode_reward,\n",
        "                \"episode\": i + 1,\n",
        "            }\n",
        "            if self.final_frame_pause and self.window and self.done:\n",
        "                self.__print_frames(frame, 2, self.window)\n",
        "            elif self.window:\n",
        "                self.__print_frames(frame, self.timestamp, self.window)\n",
        "            else:\n",
        "                self.__print_frames(frame, self.timestamp)\n",
        "        else:\n",
        "            print(f\"Test episode: {i+1} / {self.test_episodes}\")\n",
        "\n",
        "    def __print_frames(self, frame, timestamp, window=None):\n",
        " \n",
        "        clear_output(wait=True)\n",
        "        print(frame['frame'])\n",
        "        print(f\"State: {frame['state']}\")\n",
        "        print(f\"Action: {frame['action']}\")\n",
        "        print(f\"Reward: {frame['reward']}\")\n",
        "        print(f\"Action: {frame['action']}\")\n",
        "        print(f\"Eposide: {frame['episode']}\")\n",
        "        sleep(timestamp)\n",
        "\n",
        "        sleep(timestamp)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def test(pt_path):\n",
        "    model = TestModel(DQN, pt_path)\n",
        "    model.test(test_episodes=NUMBER_OF_EPISODES_FOR_TESTING, # number of test episode to execute\n",
        "               timestamp=1, # time between each frame\n",
        "               fast_testing=False, # display graphical interface or print only test informations\n",
        "               final_frame_pause=1) # time to wait after the last frame of each episode\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+---------+\n",
            "|\u001b[42mR\u001b[0m: | : :G|\n",
            "| : | : : |\n",
            "| : : : : |\n",
            "| | : | : |\n",
            "|Y| : |\u001b[35mB\u001b[0m: |\n",
            "+---------+\n",
            "  (West)\n",
            "\n",
            "State: 19\n",
            "Action: 3\n",
            "Reward: -1\n",
            "Action: 3\n",
            "Eposide: 30\n"
          ]
        }
      ],
      "source": [
        "test(pt_path)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "TPU",
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
